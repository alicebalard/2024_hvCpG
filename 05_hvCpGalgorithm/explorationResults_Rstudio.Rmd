---
title: "hvCpG_BayesAlgo_exploration"
author: "Alice Balard"
date: '`r Sys.Date()`'
output: html_document
---

```{r setup}
knitr::opts_chunk$set(echo = TRUE)
invisible(lapply(
  c("dplyr", "data.table", "matrixStats", "ggplot2", "reshape2",
    "ggrepel", "pROC", "stringr", "tibble", "viridis", "ggridges"),
  function(pkg) suppressPackageStartupMessages(library(pkg, character.only = TRUE))
))
```

# Data prep 

```{r}
##########################################################################################
## LSHTM: Maria’s data that she used for the hvCpG paper can be accessed from ing-p5 here:
# /mnt/old_user_accounts/p3/maria/PhD/Data/datasets/
#   
# Her hvCpG scripts are here:
# /mnt/old_user_accounts/p3/maria/PhD/Projects/hvCpGs/Scripts/

system.time(source("../03_prepDatasetsMaria/dataprep_MariaArrays.R")) # takes 2 minutes
```

# Reproduce Maria’s results
Maria’s paper: We defined an hvCpG in the following way:

1. in 65% of datasets in which the CpG is covered (following quality control), it has methylation variance in the top 5% of all (non-removed) CpGs.
2. is covered in at least 15 of the 30 datasets.

```{r Maria}
## Within each dataset, calculate the CpGs variance, and keep the top 5%
top5pcvar <- lapply(my_list_mat_Mariads, function(mat) {
  # Step 1: Calculate row variances
  row_variances = rowVars(mat, na.rm = T)
  df = data.frame(var=row_variances, cpg=names(row_variances))
  # Step 2
  top = top_n(df, as.integer(0.05*nrow(df)), var) ## slightly different than quantile cause keep ties
  return(top)
})

## CpGs in the top 5% variance:
cpg_counts_top <- table(unlist(lapply(top5pcvar, rownames))) %>%
  data.frame() %>% dplyr::rename("cpgs"="Var1", "all_cpgs_top5pc"="Freq")

## all covered CpGs:
cpg_counts <- table(unlist(lapply(my_list_mat_Mariads, rownames))) %>%
  data.frame() %>% dplyr::rename("cpgs"="Var1", "all_cpgs"="Freq")

## Both:
cpg_counts_full <- merge(cpg_counts, cpg_counts_top, all.x = T)
rm(cpg_counts, cpg_counts_top)

## in 65% of datasets in which the CpG is covered (following quality control), it has methylation variance in the top 5% of all (non-removed) CpGs:
hvCpGs_maria <- na.omit(cpg_counts_full[cpg_counts_full$all_cpgs_top5pc/cpg_counts_full$all_cpgs >= 0.65,])

## rounding, to mimick Maria's approach
hvCpGs_maria <- na.omit(cpg_counts_full[round(cpg_counts_full$all_cpgs_top5pc/cpg_counts_full$all_cpgs, 2) >= 0.65,])
```

Maria detected `r length(MariasCpGs$CpG)` hvCpGs. I detected `r length(hvCpGs_maria$cpgs)` hvCpGs.
We both have `r length(intersect(hvCpGs_maria$cpgs, MariasCpGs$CpG))` hvCpGs in common.

# Identify hvCpGs based on a sd multiplicative factor lambda

The proportion of CpGs than Maria found hvCpGs is p(hvCpG) = `r round(4143/406306, 4)*100`%.

## Calculate lambda 

Within each dataset k, calculate the median sd of all CpG j

```{r calcLambdas}
all_sd_jk <- sapply(my_list_mat_Mariads, function(k){
  
  ## Scale
  k = log2(k/(1-k))
  
  get_sd_k <- function(k){
    ## Calculate a vector of the row (=per CpG j) sd
    return(rowSds(k, na.rm = T))
  }
  ## Return a vector of sds, in a list for each dataset 
  return(get_sd_k(k))
})

## Plot:
df_long <- reshape2::melt(all_sd_jk, variable.name = "Vector", value.name = "SDs")

## Plot distributions of all vectors on the same graph
ggplot(df_long, aes(x = SDs, color = L1)) +
  geom_density() +
  labs(title = "Distribution of SDs accross datasets",
       x = "SDs",
       y = "Density") +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_x_sqrt()

## Calculate lambda per dataset
lambdas = sapply(all_sd_jk, function(x){quantile(x, 0.95, na.rm=T)/median(x, na.rm=T)})
names(lambdas) <- gsub(".95%", "", names(lambdas))
```

```{r}
# Histogram with kernel density
ggplot(data.frame(lambda=lambdas, dataset=names(lambdas)),
       aes(x = lambdas)) +
  geom_histogram(aes(y = after_stat(density)),
                 colour = 1, fill = "white", binwidth = .1) +
  geom_density(lwd = 1, colour = 4,
               fill = 4, alpha = 0.25)+
  theme_minimal() +
  geom_vline(xintercept = median(lambdas), col = "red")

median(lambdas) #1.878303
```

Which datasets have a higher lambda, and why? 

```{r}
df = data.frame(nind=sapply(my_list_mat_Mariads, ncol),
                dataset=names(my_list_mat_Mariads),
                lambda=lambdas, 
                tissue = sapply(strsplit(names(my_list_mat_Mariads), "_"), `[`, 1),
                ethnicity=sapply(strsplit(names(my_list_mat_Mariads), "_"), `[`, 2)) %>%
  mutate(dataset=ifelse(dataset %in% c("Blood_Cauc", "Blood_Hisp"), "Blood_Cauc_Hisp", dataset)) %>% 
  mutate(dataset=ifelse(dataset %in% c("Blood_Mexican", "Blood_PuertoRican "), "Blood_Mex_PuertoRican ", dataset)) %>% 
  mutate(dataset=ifelse(dataset %in% c("CD4+_Estonian", "CD8+_Estonian"), "CD4+_CD8+_Estonian", dataset)) %>% 
  mutate(dataset=ifelse(dataset %in% c("Saliva_Hisp", "Saliva_Cauc"), "Saliva_Hisp_Cauc", dataset))

ggplot(data = df,
       aes(x=lambda, y=nind))+
  geom_smooth(method = "lm")+
  geom_point()+
  geom_label_repel(aes(label = dataset, fill=dataset), size= 2, alpha=.8, max.overlaps = 25)+
  theme_bw()+theme(legend.position = "none")+
  scale_x_log10()
```

```{r}
## Emphasize the outliers
df_long$col = "grey"
df_long$col[df_long$L1 %in% "BulkFrontalCortex"] <- "red"

ggplot(df_long, aes(x = SDs, group = L1, fill = col)) +
  geom_density(data = df_long[!df_long$L1 %in% "BulkFrontalCortex",], alpha = .5) +
  geom_density(data = df_long[df_long$L1 %in% "BulkFrontalCortex",], alpha = .6) +
  scale_fill_manual(values = c("grey", "red"))+
  labs(title = "Distribution of SDs accross datasets",subtitle = "Red=BulkFrontalCortex",
       x = "SDs",
       y = "Density") +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_x_sqrt()
```

```{r}
## Emphasize the outliers
df_long$col = "grey"
df_long$col[df_long$L1 %in% "Blood_Cauc"] <- "red"

ggplot(df_long, aes(x = SDs, group = L1, fill = col)) +
  geom_density(data = df_long[!df_long$L1 %in% "Blood_Cauc",], alpha = .5) +
  geom_density(data = df_long[df_long$L1 %in% "Blood_Cauc",], alpha = .6) +
  scale_fill_manual(values = c("grey", "red"))+
  labs(title = "Distribution of SDs accross datasets",subtitle = "Red=Blood_Cauc",
       x = "SDs",
       y = "Density") +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_x_sqrt()
```

```{r}
## Emphasize the outlier
df_long$col = "grey"
df_long$col[df_long$L1 %in% "Blood_Hisp"] <- "red"

ggplot(df_long, aes(x = SDs, group = L1, fill = col)) +
  geom_density(data = df_long[!df_long$L1 %in% "Blood_Hisp",], alpha = .5) +
  geom_density(data = df_long[df_long$L1 %in% "Blood_Hisp",], alpha = .6) +
  scale_fill_manual(values = c("grey", "red"))+
  labs(title = "Distribution of SDs accross datasets",subtitle = "Red=Blood_Hisp",
       x = "SDs",
       y = "Density") +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_x_sqrt()
```

# Maximum likelihood analysis

The equation is:

$$
\log\left(P(M_j)\right) = \sum_{i=1}^{n} \log\left( \sum_{Z_j=0}^{1} \left( \sum_{Z_{j,k}=0}^{1} P(M_{i,j} \mid Z_{j,k}) \times P(Z_{j,k} \mid Z_j) \right) \times P(Z_j) \right)
$$

## Examine weird values 

The transformation lead to weird values:

```{r investigate_weird_transfo}
# my_list_mat_Mariads$Blood_Hisp[rownames(my_list_mat_Mariads$Blood_Hisp) %in% "cg23089912",1:10]
# scaled_list_mat$Blood_Hisp[rownames(scaled_list_mat$Blood_Hisp) %in% "cg23089912",1:10]
# 
# test <- readRDS("/mnt/old_user_accounts/p3/maria/PhD/Data/datasets/GEO/Raw_cleaned_beta_matrices_GEO/Blood_Hisp")
# test[rownames(test) %in% "cg23089912",5:10]
# 
# test <- readRDS("/mnt/old_user_accounts/p3/maria/PhD/Data/datasets/GEO/BMIQ + 10 PCs + age + sex OUTLIERS REMOVED/Blood_Hisp.RDS")
# test[rownames(test) %in% "cg23089912",5:10]
# # GSM1870986
# # 0.00000000000000001124546 --> give extreme value in scaling, and p dnorm=0
```

The zero in 8h sample was weirdly transformed. The cleaned values are very different from the original. Is that expected? 

## Optimisation over multiple CpGs

Run on LSHTM server (need to be outside of RStudio): 
source("S01_run_hvCpGdetection_Marias.R") ## for different datasets

### "Nelder-Mead" or "L-BFGS-B" optimisation method?

```{r}
load("/home/alice/2024_hvCpG/05_hvCpGalgorithm/resultsDir/Mariads/results_Nelder_Mead_my_list_mat_Mariads_hvCpGandControls_0_95p0_0_65p1.RData")
A <- results_Nelder_Mead_my_list_mat_Mariads_hvCpGandControls_0_95p0_0_65p1 %>% data.frame %>% tibble::rownames_to_column("CpG") %>% dplyr::rename(alpha_NM_0.95="alpha_hat")

load("/home/alice/2024_hvCpG/05_hvCpGalgorithm/resultsDir/Mariads/results_L_BFGS_B_my_list_mat_Mariads_hvCpGandControls_0_95p0_0_65p1.RData")
B <- results_L_BFGS_B_my_list_mat_Mariads_hvCpGandControls_0_95p0_0_65p1 %>% 
  data.frame %>% tibble::rownames_to_column("CpG") %>% dplyr::rename(alpha_LB_0.95="alpha_hat")

merge(A, B, by = "CpG") -> results_bothMethods
results_bothMethods %>% mutate(ishvCpG = ifelse(CpG %in% MariasCpGs$CpG, 
                                                  "hvCpG in Maria's study", "mQTL matching controls")) %>% ggplot(aes(x=alpha_LB_0.95, y=alpha_NM_0.95, fill = ishvCpG)) +
  geom_abline(slope = 1) +
  geom_point(pch=21, size = 3, alpha = .8) +
  theme_bw()
```

Some CpGs have an alpha of 0 for LBB algorithm and different alpha for NM

```{r}
results_bothMethods[round(results_bothMethods$alpha_NM_0.95, 1) != round(results_bothMethods$alpha_LB_0.95, 1),] %>% mutate(ishvCpG = ifelse(CpG %in% MariasCpGs$CpG, "hvCpG in Maria's study", "mQTL matching controls")) %>% melt() %>%
  ggplot(aes(x=CpG, y=value, fill = variable)) +
  geom_point(pch=21, size = 3, alpha = .8) +
  facet_grid(ishvCpG~.) + theme_bw() + theme(axis.text.x = element_blank())
```

The probability of zero seems to be an artefact of the "L-BFGS-B" optimisation method, so we choose "Nelder-Mead".

## ROC curves to define p0 and p1 matching Maria's approach

$$
p0 = P(Z_{j,k}=0 \mid Z_j=0) \\
p1 = P(Z_{j,k}=1 \mid Z_j=1)
$$

p1 = 0.65 in Maria's approach (to be a hvCpG, it needs to be hypervariable in >65% of the datasets)

```{r}
# Step 1: Read results and compute AUC + threshold
files <- list.files(
  "resultsDir/Mariads",
  pattern = "^results_Nelder_Mead_.*\\.RData$",
  full.names = TRUE
)

auc_df <- tibble(p0=double(), p1=double(), AUC=double(), threshold=double())
roc_list <- list()

for (f in files) {
  if (file.info(f)$size == 0) next
  nm <- load(f)
  res <- get(nm)
  
  pstr <- str_match(f, "_(\\d+(?:_\\d+)?)p0_(\\d+(?:_\\d+)?)p1")[, 2:3]
  p0 <- as.numeric(gsub("_", ".", pstr[1]))
  p1 <- as.numeric(gsub("_", ".", pstr[2]))
  label <- sprintf("p0=%.2f p1=%.2f", p0, p1)
  
  df <- as.data.frame(res) %>% rownames_to_column("CpG")
  colnames(df)[2] <- "alpha"
  
  truth <- ifelse(df$CpG %in% MariasCpGs$CpG, 1, 0)
  roc_obj <- try(roc(truth, df$alpha, quiet=TRUE), silent=TRUE)
  if (inherits(roc_obj, "try-error")) next
  
  aucv <- as.numeric(auc(roc_obj))
  thr <- coords(roc_obj, "best", ret = "threshold", transpose = FALSE)[["threshold"]]
  auc_df <- auc_df %>% add_row(p0, p1, AUC=aucv, threshold=thr)
  
  pts <- coords(roc_obj, "all", ret=c("sensitivity","specificity"),
                transpose = FALSE) %>%
    as_tibble() %>%
    mutate(FPR = 1 - specificity, TPR = sensitivity, label=label)
  roc_list[[label]] <- pts
}

roc_all <- bind_rows(roc_list)

# Step 2: Plot ROC curves
roc_plot <- ggplot(roc_all, aes(FPR, TPR, group=label, color=label)) +
  geom_line(size=0.8, alpha=0.6) +
  geom_abline(slope=1, linetype="dashed", color="gray50") +
  labs(title="ROC Curves for Different (p0,p1)",
       x="False Positive Rate", y="True Positive Rate") +
  theme_minimal() + theme(legend.position="none")

# Step 3: AUC heatmap with thresholds
best <- auc_df %>% slice_max(AUC, n=1)

heatmap_plot <- ggplot(auc_df, aes(p0, p1, fill=AUC)) +
  geom_tile(color="white") +
  geom_text(aes(label=round(threshold,2)),
            size=3, color="black", check_overlap = TRUE) +
  # Outline best tile
  geom_tile(data = best, aes(p0, p1), fill = NA, color = "red", linewidth = 1.5) +
  scale_fill_viridis_c(option = "plasma") +
  labs(title="AUC & Optimal Threshold per (p0,p1)",
       x="p0", y="p1") +
  theme_minimal()
# Step 4: Show both plots
print(roc_plot)
print(heatmap_plot)
```

The best threshold ploted in tiles is the alpha probability that gives the best sensitivity-specificity trade-off. It's derived using a statistical rule (Youden's J: maximizes (Sensitivity + Specificity – 1), giving the best overall balance between true positives and true negatives). It classifies a CpG as hypervariable in Maria's approach if alpha > threshold 

### Conclusions: 

* Best AUC = for p0 (true negative) is quite low in Maria's data (0.45)
* Threshold for this best AUC = : Classify a CpG as hvCpG in Maria's data if alpha ≥ 


## Understanding discrepancies between method

I think that it's due to a high rate of false positive accepted in Maria's approach. The sensibility is high (p1, true positive = 65%), but the specificity is low (we get a lot of hvCpG that have a low probability to be one in my method). I think it's because it accepts as hvCpG CpG with a very low variance in certain datasets (up to 35%), while my approach gain power with each dataset. Let's test that hypothesis.

```{r}
## 10 hvCpG with highest alpha + match controls
## 10 hvCpG with lowest alpha + match controls

# 1️⃣ Make the alpha_hat a data frame with a `hvCpG_name` column
alpha_df <- data.frame(
  hvCpG_name = rownames(results_Nelder_Mead_my_list_mat_Mariads_hvCpGandControls_0_95p0_0_65p1),
  alpha_hat = results_Nelder_Mead_my_list_mat_Mariads_hvCpGandControls_0_95p0_0_65p1
)

# 2️⃣ Merge on hvCpG_name
merged_df <- merge(sub_cistrans_GoDMC_hvCpG_matched_control,alpha_df,
  by = "hvCpG_name",all.x = TRUE) ; rm(alpha_df)

# Rows with minimum and maximum values of a variable
bottom10 <- merged_df %>% slice_min(alpha_hat, n = 10) %>% mutate(which="bottom10alpha")
top10 <- merged_df %>% slice_max(alpha_hat, n = 10) %>% head(10) %>% mutate(which="top10alpha")
df <- rbind(bottom10, top10)

# 3️⃣ Variance per data frame
myListVar <- lapply(my_list_mat_Mariads, function(mat) {
  row_variances = rowVars(mat, na.rm = T)
  df = data.frame(var=row_variances, cpg=names(row_variances))
  })
  
myListVar$Blood_Cauc$cpg %in% c(df$hvCpG_name, df$controlCpG_name) %>% head


# ✅ Result: same rows as sub_cistrans_GoDMC_hvCpG_matched_control, with `alpha_hat` added.
head(merged_df)
```


```{r}




# mutate(ishvCpG = ifelse(CpG %in% MariasCpGs$CpG, 
#                                                   "1500 hvCpG in Maria's study", "1500 mQTL matching controls"))

my_list_mat_Mariads_Subset <- lapply(my_list_mat_Mariads, function(x) x[rownames(x) %in% sub_cistrans_GoDMC_hvCpG_matched_control,])

# 📊 1. Calculate Per-CpG Variance

# Calculate per-CpG variance across datasets
var_list <- lapply(my_list_mat_Mariads_Subset3000, function(mat) rowSds(mat, na.rm = TRUE))



# 1. Determine the full set of CpG names across all matrices
all_cpgs <- unique(unlist(lapply(my_list_mat_Mariads_Subset, rownames)))

# 2. For each matrix, ensure it has all CpGs, filling missing rows with NA
aligned_list <- lapply(my_list_mat_Mariads_Subset3000, function(mat) {
  # Create empty matrix with full CpG set and same columns
  full_mat <- matrix(NA_real_, nrow = length(all_cpgs), ncol = ncol(mat),
                     dimnames = list(all_cpgs, colnames(mat)))
  # Insert existing data
  intersecting <- intersect(rownames(mat), all_cpgs)
  full_mat[intersecting, ] <- mat[intersecting, , drop = FALSE]
  full_mat
})

# 3. Optional: check that all aligned matrices have same row count
unique(sapply(aligned_list, nrow))  == length(all_cpgs)

test3000CpGsvec[!test3000CpGsvec %in% all_cpgs] %in% sub1500$controlCpG_name
```


```{r}
# 📊 1. Calculate Per-CpG Variance and Coverage

# Calculate per-CpG variance across datasets
var_list <- lapply(my_list_mat_Mariads_Subset3000, function(mat) rowSds(mat, na.rm = TRUE))
# Convert to matrix
var_mat <- do.call(cbind, var_list)


# Calculate per-CpG coverage (number of non-NA values) across datasets
cpg_coverage <- sapply(my_list_mat_Mariads, function(mat) rowCounts(!is.na(mat)))

# Convert both to numeric matrices:
var_mat <- simplify2array(cpg_variance)  # ensures numeric matrix
cov_mat <- simplify2array(cpg_coverage)

# 🔍 2. Visualize Methylation Variance and Coverage
# a. Heatmap of Per-CpG Variance
library(ComplexHeatmap)
library(circlize)

Heatmap(var_mat,
        name = "Variance",
        col = colorRamp2(c(min(var_mat), mean(var_mat), max(var_mat)), c("white", "yellow", "red")),
        show_row_names = FALSE, show_column_names = TRUE)

# b. Heatmap of Per-CpG Coverage
Heatmap(cpg_coverage,
        name = "Coverage",
        show_row_names = FALSE,
        show_column_names = FALSE,
        cluster_rows = TRUE,
        cluster_columns = TRUE,
        col = colorRampPalette(c("white", "blue"))(100))

# 📈 3. Compare Detection Methods
# Assuming maria_flags is a vector indicating hvCpG status from Maria's method:

# Create a data frame for visualization
comparison_df <- data.frame(
  CpG = rownames(cpg_variance),
  Maria = maria_flags,
  Alice = ifelse(alice_alpha > 0.5, "hvCpG", "Control")
)

# Create a table of counts
table(comparison_df$Maria, comparison_df$Alice)

# 📊 4. Visualize Distribution of Alice's Alpha Values
library(ggplot2)

ggplot(comparison_df, aes(x = Alice, y = alice_alpha)) +
  geom_boxplot(aes(fill = Alice)) +
  labs(title = "Distribution of Alice's Alpha Values",
       x = "Detection Method",
       y = "Alpha Value") +
  theme_minimal()

# 🧩 Interpretation
# Heatmaps: Identify CpGs with high variance or low coverage that may contribute to discrepancies between methods.
# 
# Comparison Table: Understand the overlap and differences between the hvCpG sets identified by Maria and Alice.
# 
# Boxplot: Assess how Alice's alpha values differ between CpGs flagged by Maria and those not flagged.
```


# Subset Maria's data following Atlas data structure (less N/dataset) to check power of detection in Atlas data:          

3 tries for (p0, p1): 

* c(0.45, 0.6): best AUC, closest to Maria's data; low specificity and mid sensitivity                                    * c(0.95, 0.65): our original idea, high specificity but mid sensitivity                                                  * c(0.9, 0.9): high specificity and sensitivity       

```{r}
makeMyPlots <-function(p0p1){
  df1 = get(paste0("results_Nelder_Mead_my_list_mat_Mariads_test3000CpGsvec_", p0p1))
  df2 = get(paste0("results_Nelder_Mead_my_list_mat_Mariads_mimicAtlas_test3000CpGsvec_", p0p1))
  
  p_str = str_match(p0p1, "(\\d+_\\d+)p0_(\\d+_\\d+)p1")[, 2:3]
  p0 = gsub("_", ".", p_str[1])
  p1 = gsub("_", ".", p_str[2])
  label = paste0("p0=", p0, ", p1=", p1)
  
  plot1 = data.frame(df1, df2) %>%
    tibble::rownames_to_column("CpG") %>%
    mutate(ishvCpG = ifelse(CpG %in% MariasCpGs$CpG, "1500 hvCpG in Maria's study", "1500 mQTL matching controls")) %>%
    dplyr::rename(alpha_normalDS = alpha_hat, alpha_mimicAtlas = alpha_hat.1) %>% 
    ggplot(aes(x = alpha_normalDS, y = alpha_mimicAtlas)) +
    geom_point(aes(fill=ishvCpG), pch =21, size =3, alpha = .3) + 
    geom_abline(slope = 1, linetype = "dashed") + theme_bw() +
    ggtitle(label = label)
  
  # === 1) Prepare the ORIGINAL ===
  df_original = df1 %>%
    data.frame() %>%
    tibble::rownames_to_column("CpG") %>%
    mutate(ishvCpG = ifelse(
      CpG %in% MariasCpGs$CpG,
      "1500 hvCpG in Maria's study",
      "1500 mQTL matching controls"
    ),
    source = "original") %>%
    melt()
  
  # === 2) Prepare the MIMIC ===
  df_mimic = df2 %>%
    data.frame() %>%
    tibble::rownames_to_column("CpG") %>%
    mutate(ishvCpG = ifelse(
      CpG %in% MariasCpGs$CpG,
      "1500 hvCpG in Maria's study",
      "1500 mQTL matching controls"
    ),
    source = "mimicAtlas") %>%
    melt()
  
  # === 3) Combine ===
  df_combined = bind_rows(df_original, df_mimic)
  
  # === 4) Plot ===
  plot2 = ggplot(df_combined, aes(x = value, y = ishvCpG, fill = ..x..)) +
    geom_density_ridges_gradient(scale = 3, rel_min_height = .01) +
    facet_wrap(~source, ncol = 1) +
    scale_fill_viridis(name = "alpha", option = "C") +
    theme_bw() +
    theme(axis.title.y = element_blank()) +
    labs(title = "Probability alpha of being hypervariable in my algorithm",
         subtitle = "Original data of Maria vs mimic Atlas structure")
  
  print(plot1)
  print(plot2)
}
```


## 1. low specificity and mid sensitivity (p0=0.45 and p1=0.6)
### best AUC, my algorithm with the closest proximity to Maria's approach)

```{r}
load("resultsDir/results_Nelder_Mead_my_list_mat_Mariads_mimicAtlas_test3000CpGsvec_0_45p0_0_6p1.RData")
makeMyPlots(p0p1 = "0_45p0_0_6p1")
```

## 2. high specificity and mid sensitivity (p0=0.95 and p1=0.65)
### high true negative rate, i.e. high specificity)

```{r}
load("resultsDir/results_Nelder_Mead_my_list_mat_Mariads_mimicAtlas_test3000CpGsvec_0_95p0_0_65p1.RData")
makeMyPlots(p0p1 = "0_95p0_0_65p1")
```

## 3. high specificity and sensitivity (p0=0.9 and p1=0.9)

```{r}
load("resultsDir/results_Nelder_Mead_my_list_mat_Mariads_mimicAtlas_test3000CpGsvec_0_9p0_0_9p1.RData")
makeMyPlots(p0p1 = "0_9p0_0_9p1")
```

# Notes:

lambdas are defined based on a 5% threshold, even if alpha is not. How to not hardcode them? MCMC on lambda? Link with alpha?

# The analysis is complete.
