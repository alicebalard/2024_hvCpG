## hvCpG algorithm (batched HDF5 loading)
## Alice Balard — Updated Aug 2025

###########
## Setup ##
###########

quiet_library <- function(pkg) {
  # Check if installed
  installed <- requireNamespace(pkg, quietly = TRUE)
  
  # Install if missing
  if (!installed) {
    if (!requireNamespace("BiocManager", quietly = TRUE)) {
      suppressMessages(suppressWarnings(
        install.packages("BiocManager", repos = "https://cloud.r-project.org", quiet = TRUE)
      ))
    }
    
    cran_pkgs <- suppressMessages(available.packages(repos = "https://cloud.r-project.org"))
    if (pkg %in% rownames(cran_pkgs)) {
      suppressMessages(suppressWarnings(
        install.packages(pkg, repos = "https://cloud.r-project.org", quiet = TRUE)
      ))
    } else {
      suppressMessages(suppressWarnings(
        BiocManager::install(pkg, ask = FALSE, update = FALSE, quiet = TRUE)
      ))
    }
  }
  
  # Load silently
  suppressPackageStartupMessages(
    suppressMessages(
      suppressWarnings(
        library(pkg, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)
      )
    )
  )
  
  # Get version after loading
  version <- as.character(utils::packageVersion(pkg))
  
  # Print only our message
  cat(sprintf("Load package %s v%s\n", pkg, version))
}

quiet_library_all <- function(pkgs) {
  invisible(lapply(pkgs, quiet_library))
}

quiet_library_all(c("dplyr", "data.table", "matrixStats", "reshape2", "ggrepel",
                    "parallel", "rhdf5", "IlluminaHumanMethylation450kanno.ilmn12.hg19", "tidyr", 
                    "ggplot2", "GenomicRanges", "rtracklayer"))
## NB: not all libraries are necessary; to clean when packaging

###############
## Data load ##
###############

prepData <- function(analysis, dataDir) {   
  if (grepl("MariasarraysREDUCED", analysis)) {
    x <- sub("MariasarraysREDUCED", "", analysis)
    basepath <- file.path("/home/alice/arraysh5_reducedMimicAtlas", x)
    metapath <- file.path(basepath, "all_metadata.tsv")
    if (!file.exists(metapath)) {
      stop("❌ Run 03_prepDatasetsMaria/S02.prepare_REDUCED_arrays_mimicAtlas.py")
    }
    metadata <- read.table(metapath, sep = "\t", header = TRUE)
    medsd_lambdas <- read.table(file.path(basepath, "all_medsd_lambda.tsv"), sep = "\t", header = TRUE)
    cpg_names_all <- h5read(file.path(basepath, "all_scaled_matrix.h5"), "cpg_names")
    h5file <- file.path(basepath, "all_scaled_matrix.h5")        
  } else {
    metadata <- read.table(file.path(dataDir, "sample_metadata.tsv"), sep = "\t", header = TRUE)
    medsd_lambdas <- read.table(file.path(dataDir, "all_medsd_lambda.tsv"), sep = "\t", header = TRUE)
    cpg_names_all <- h5read(file.path(dataDir, "all_matrix_noscale.h5"), "cpg_names")
    h5file <- file.path(dataDir, "all_matrix_noscale.h5")
  }
  return(list(
    metadata = metadata,
    medsd_lambdas = medsd_lambdas,
    cpg_names_all = cpg_names_all,
    h5file = h5file
  ))
}

###########################################
## Likelihood function for a given CpG j ##
###########################################

getLogLik_oneCpG_optimized_fast <- function(Mdf, metadata, dataset_groups, ds_params, p0, p1, alpha) {
  
  samples <- metadata$sample
  datasets <- unique(metadata$dataset)
  
  # Precompute p0/p1 matrix and mixture probs
  p0p1_mat <- matrix(c(p0, 1 - p1, 1 - p0, p1), nrow = 2, byrow = TRUE)
  proba_hvCpG_vec <- c(1 - alpha, alpha)
  
  log_P_Mj <- 0
  
  for (k in datasets) {
    Mij_vals <- as.numeric(Mdf[dataset_groups[[k]], , drop = FALSE])
    if (length(Mij_vals) < 3 || all(is.na(Mij_vals))) next
    
    # Precompute mean and SDs
    mu_jk <- mean(Mij_vals, na.rm = TRUE)
    
    ## Cal precomputed sds for both cases
    params =  ds_params[k, ]

    # Vectorized density per mixture component
    norm_probs <- matrix(0, nrow = length(Mij_vals), ncol = 2)
    norm_probs[,1] <- dnorm(Mij_vals, mu_jk, params$sd0)
    norm_probs[,2] <- dnorm(Mij_vals, mu_jk, params$sd1)
    
    # Compute zjk_probs safely
    zjk_probs <- array(0, dim = c(length(Mij_vals), 2, 2))
    for (zjk in 0:1) {
      zjk_probs[,, zjk+1] <- cbind(
        norm_probs[, zjk+1] * p0p1_mat[zjk+1,1],
        norm_probs[, zjk+1] * p0p1_mat[zjk+1,2]
      )
    }
    
    # Sum across latent states and mixture
    col_sums <- apply(zjk_probs, c(1,3), sum)
    dataset_loglik <- sum(log(rowSums(col_sums %*% proba_hvCpG_vec)))
    if (!is.finite(dataset_loglik)) dataset_loglik <- 0
    
    log_P_Mj <- log_P_Mj + dataset_loglik
  }
  
  return(log_P_Mj)
}

################################
## Optimisation per CpG ##
################################

runOptim1CpG_gridrefine <- function(Mdf, metadata, dataset_groups, ds_params, p0, p1) {
  # Step 1. Coarse grid search (0, 0.05, 0.1...)
  grid <- seq(0, 1, length.out = 21)
  logliks <- vapply(grid, function(a) {
    getLogLik_oneCpG_optimized_fast(Mdf, metadata, dataset_groups, ds_params, p0, p1, a)
  }, numeric(1))
  
  best_idx <- which.max(logliks)
  alpha_start <- grid[best_idx]

  # Step 2. Local refinement with Brent, only in neighborhood
  lower <- ifelse(best_idx == 1, 0, grid[best_idx - 1])
  upper <- ifelse(best_idx == length(grid), 1, grid[best_idx + 1])
  
  resOpt <- optim(
    par = alpha_start,
    fn = function(alpha) {
      getLogLik_oneCpG_optimized_fast(Mdf, metadata, dataset_groups, ds_params, p0, p1, alpha)
    },
    method = "Brent",
    lower = lower, upper = upper,
    control = list(fnscale = -1)
  )
  return(resOpt$par)
}

#########################################
## Batch loading + parallel processing ##
#########################################

getAllOptimAlpha_parallel_batch_fast <- function(cpg_names_vec, NCORES, p0, p1, prep, batch_size = 1000) {
  metadata       <- prep$metadata
  cpg_names_all  <- prep$cpg_names_all
  h5file         <- prep$h5file
  medsd_lambdas  <- prep$medsd_lambdas
  
  ## Precompute dataset-level parameters
  ds_params = medsd_lambdas %>%
    dplyr::select(dataset, median_sd, lambda) %>%
    dplyr::mutate(sd0 = pmax(median_sd, 1e-4),
                  sd1 = pmax(lambda * median_sd, 1e-4)) %>%
    as.data.frame()
  rownames(ds_params) = ds_params$dataset
  
  ## Build a list of row indices grouped by dataset
  dataset_groups <- split(seq_len(nrow(metadata)), metadata$dataset)

  # Read sample names once
  samples <- h5read(h5file, "samples")
  
  # Map CpG names to indices
  cpg_indices <- match(cpg_names_vec, cpg_names_all)
  if (anyNA(cpg_indices)) {
    stop("Some CpG names not found in HDF5: ", paste(cpg_names_vec[is.na(cpg_indices)], collapse = ", "))
  }
  
  # Container for results
  all_results <- vector("list", length(cpg_indices))
  
  # Split into batches
  batches <- split(cpg_indices, ceiling(seq_along(cpg_indices) / batch_size))
  
  for (b in seq_along(batches)) {
    message(sprintf(
      "📦 Loading batch %d / %d (%d CpGs) at %s",
      b, length(batches), length(batches[[b]]), Sys.time()
    ))
    
    # Load block of matrix (samples × CpGs) with rhdf5::h5read direct slice
    col_batches <- batches[[b]]
    M_batch <- h5read(
      file   = h5file,
      name   = "matrix",
      index  = list(NULL, col_batches)  # NULL = all rows, subset columns
    )
    
    # Assign dimnames
    rownames(M_batch) <- samples
    colnames(M_batch) <- cpg_names_all[col_batches]
    
    # Reorder rows to match metadata
    M_batch <- M_batch[metadata$sample, , drop = FALSE]
    
    sample_to_dataset <- setNames(metadata$dataset, metadata$sample)
    
    # Split CpGs into chunks (not one per worker)
    idx_split <- split(seq_len(ncol(M_batch)), cut(seq_len(ncol(M_batch)), NCORES, labels = FALSE))
    
    # Run in parallel over chunks
    chunk_results <- mclapply(idx_split, function(idx) {
      sapply(idx, function(i) {
        Mdf <- M_batch[, i, drop = FALSE]
        
        # Require at least 3 datasets with data
        datasets_present <- unique(sample_to_dataset[names(Mdf[!is.na(Mdf), ])])
        if (length(datasets_present) < 3) return(NA_real_)
        
        res <- tryCatch(
          runOptim1CpG_gridrefine(Mdf = Mdf, metadata = metadata, dataset_groups = dataset_groups,
                            ds_params = ds_params, p0 = p0, p1 = p1),
          error = function(e) NA_real_
        )
        return(res)
      })
    }, mc.cores = NCORES)
    
    batch_results <- unlist(chunk_results, use.names = FALSE)
    
    # Store results (original positions)
    all_results[match(batches[[b]], cpg_indices)] <- batch_results
  }
  
  # Build final matrix
  my_matrix <- matrix(unlist(all_results), ncol = 1)
  rownames(my_matrix) <- cpg_names_vec
  colnames(my_matrix) <- "alpha"
  
  return(my_matrix)
}

######################
## Top-level runner ##
######################
##   dataDir: if  testLocalPC = "~/Documents/Project_hvCpG/10X/",
##   Atlas10X = "/SAN/ghlab/epigen/Alice/hvCpG_project/data/WGBS_human/AtlasLoyfer/10X/",
##   Maria = "/home/alice/arraysh5"

runAndSave_fast <- function(analysis, cpg_names_vec, resultDir, NCORES, p0, p1, overwrite = FALSE, batch_size = 1000, dataDir, skipsave=FALSE) {
  
  t <- Sys.time()
  prep <- prepData(analysis, dataDir)
  message("Preparing the data took ", round(Sys.time() - t), " seconds")
  
  obj_name <- paste0("results_", analysis, "_", length(cpg_names_vec), "CpGs_", p0, "p0_", p1, "p1")
  obj_name <- gsub("[^[:alnum:]_]", "_", obj_name)
  
  if (!grepl("/$", resultDir)) resultDir <- paste0(resultDir, "/")
  if (!dir.exists(resultDir)) {
    dir.create(resultDir, recursive = TRUE)
    message("New result directory ", resultDir, " created")
  }
  
  file_name <- paste0(resultDir, obj_name, ".RData")
  if (!overwrite && file.exists(file_name)) {
    message("⚠️ File already exists: ", file_name)
    return(invisible(NULL))
  }
  
  # Run batch + parallel processing
  result <- getAllOptimAlpha_parallel_batch_fast(
    cpg_names_vec = cpg_names_vec, NCORES = NCORES,
    p0 = p0, p1 = p1, prep = prep, batch_size = batch_size
  )
  
  assign(obj_name, result, envir = .GlobalEnv)
  
  if (!skipsave) {
    message("Saving to file: ", file_name)
    save(list = obj_name, file = file_name, envir = .GlobalEnv)
    message("💾 Result saved successfully.")
  }
}