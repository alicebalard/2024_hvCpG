---
title: "Analysis Report: Identify hvCpGs in arrays"
author: "Alice Balard"
date: "`r Sys.Date()`"
output: html_document
---

```{r}
library(dplyr)
library(data.table)
library(matrixStats)
library(ggplot2)
library(reshape2)

## Files untared from Maria's datasets folder
folder_path <- "/SAN/ghlab/pophistory/Alice/hvCpG_project/data/arrays_human/30datasetsMaria"

# List all .rds files in the folder
rds_files <- list.files(path = folder_path, pattern = "\\.RDS$", full.names = TRUE)

# Read all .rds files into a list and convert each to a matrix
## NB: needs big memory allocation to translate into matrix
## qrsh -l tmem=50G,h_vmem=50G,h_rt=1:00:00
rerun=F
if (rerun==T){
    rds_list_mat <- lapply(rds_files, function(file) {
        as.matrix(readRDS(file))
    })
    ## Name the list elements by file names (without extensions)
    names(rds_list_mat) <- gsub("\\.RDS$", "", basename(rds_files))
    ## Save in RDS for now
    saveRDS(rds_list_mat, "/SAN/ghlab/pophistory/Alice/hvCpG_project/data/arrays_human/rds_list_27.RDS")
} else {
    rds_list_mat <- readRDS("/SAN/ghlab/pophistory/Alice/hvCpG_project/data/arrays_human/rds_list_27.RDS")
}

##     GSM1870951 GSM1870952 GSM1870953
## cg00000957 0.88650894 0.87378629 0.90568321
## cg00001349 0.88900837 0.84949440 0.89971964
## cg00001583 0.06226518 0.05918149 0.05000655

########################
## Part 1: preprocessing

## tbc see Maria's code

## nb add Gambian datasets * 3

## Identify the array background
all_cpgs <- unlist(lapply(rds_list_mat, rownames))
cpg_counts <- table(all_cpgs)
common_cpgs <- names(cpg_counts[cpg_counts >= 15])
rm(all_cpgs)

# Keep only the array background                                                                                                  
for (i in 1:length(rds_list_mat)){
    rds_list_mat[[i]] <- rds_list_mat[[i]][rownames(rds_list_mat[[i]]) %in% common_cpgs,]
}

## so far (before adding 3 datasets and proper data prep) I have an array of CpGs covered:
print(paste0("We consider ", length(common_cpgs), " CpGs (in Maria's paper: array background 406 306 CpGs covered in at least 15 of the 30 datasets used in this study)"))
```

## Part 2: identify hvCpGs based on thresholds (reproduce Maria's results)

```{r}
## Within each dataset, calculate the CpGs variance, and keep the top 5%
filtered_matrices <- lapply(rds_list_mat, function(mat) {
                                        # Step 1: Calculate row variances
    row_variances <- rowVars(mat, na.rm = T)
    
                                        # Step 2: Find the threshold for the top 5% variances
    threshold <- quantile(row_variances, probs = 0.95)
    
                                        # Step 3: Subset the matrix to keep only rows with variance above the threshold
    mat[row_variances > threshold, , drop = FALSE]
})

## Select hvCpG as top 5 CpG covered in >= 65% of the ds
all_cpgs_top <- unlist(lapply(filtered_matrices, rownames))
cpg_counts_top <- table(all_cpgs_top)

cpg_counts <- data.frame(cpg_counts) %>% rename("cpgs"="all_cpgs", "all_cpgs"="Freq")
cpg_counts_top <- data.frame(cpg_counts_top) %>% rename("cpgs"="all_cpgs_top", "all_cpgs_top"="Freq")

cpg_counts_full <- merge(cpg_counts, cpg_counts_top)
cpg_counts_full <- cpg_counts_full[cpg_counts_full$all_cpgs_top >= 0.65*cpg_counts_full$all_cpgs, , drop = FALSE]

MariasCpGs <- read.csv("/SAN/ghlab/pophistory/Alice/hvCpG_project/data/arrays_human/4143hvCpGMaria_list.txt")
print(paste0("Maria detected ", length(MariasCpGs$CpG), " hvCpGs (NB: 3 more datasets in LSHTM)"))

print(paste0("We detected ", nrow(cpg_counts_full), " hvCpGs"))

print(paste0("We both have ", length(intersect(as.character(cpg_counts_full$cpgs), MariasCpGs$CpG))," hvCpGs in common"))
```

## Part 3: identify hvCpGs based on a sd multiplicative factor lambda

### 3.1. Test different lambda to reach the same proportion of CpGs than Maria

```{r}
## We fix some parameters:
thr = 0.65 ## is a CpG hv in at least 65% of datasets?
## p(Zk=1|Z=1)

## lambda is the multiplicative factor of standard deviation to be a hvCpG
findLamba <- function(lambda, thr = 0.65){
    ## Within each dataset k, calculate the hvCpG
    hv_k <- lapply(rds_list_mat, function(k){
        ## Scale
        k = log2(k/(1-k))

        ## Calculate the row (=per CpG j) sd
        sd_j <- rowSds(k, na.rm = T)

        ## Calculate the median sd in dataset k 
        sigma_k <- median(sd_j, na.rm = T)

        ## Is the sd higher than lambda x sigma_k?
        names(sd_j[sd_j >= lambda * sigma_k])
    })

    ## a hv in dataset k is a hv overall if hv in at least threshold
    find_common_elements <- function(vector_list, threshold) {
        ## Combine all vectors into a single vector
        all_elements <- unlist(vector_list)

        ## Count occurrences of each unique element
        element_counts <- table(all_elements)
        
        ## Calculate the threshold count
        threshold_count <- length(vector_list) * threshold
        
        ## Find elements that meet or exceed the threshold
        common_elements <- names(element_counts[element_counts >= threshold_count])
        
        return(common_elements)
    }

    hv <- find_common_elements(vector_list = hv_k, threshold = thr)

    ## Calculate Pr(Zj=1) i.e. the probability that a CpG j is a hvCpG
    ## for lambda = 5, p=0.1894192 %
    data.frame(lambda=lambda, phvCpG=length(hv) / length(common_cpgs))
}

print(paste0("Maria's results:p(hvCpG) = ", 4143/406306))

print("my results:")

results <- lapply(seq(1, 5, by = 0.5), findLamba)
result_df <- do.call(rbind, results)

result_df

ggplot(result_df, aes(x=lambda, y=phvCpG)) +
    geom_hline(yintercept = 4143/406306, color = "red") +
    geom_point(color="blue") +
    geom_line(color="blue") +
    ggtitle("p(hvCpG) vs lambda") +
    xlab("lambda") +
    ylab("p(hvCpG)") +
    theme_minimal()
```

### 3.2. Calculate lambda directly

```{r}
## Within each dataset k, calculate the median sd of all CpG j

all_sd_jk <- sapply(rds_list_mat, function(k){
    ## Scale
    k = log2(k/(1-k))

    get_sd_k <- function(k){
        ## Calculate a vector of the row (=per CpG j) sd
        return(rowSds(k, na.rm = T))
    }
    ## Return a vector of sds, in a list for each dataset 
    return(get_sd_k(k))
})

## Plot:
df_long <- melt(all_sd_jk, variable.name = "Vector", value.name = "SDs")

## Plot distributions of all vectors on the same graph
ggplot(df_long, aes(x = SDs, color = L1)) +
    geom_density() +
    labs(title = "Distribution of SDs accross datasets",
         x = "SDs",
         y = "Density") +
    theme_minimal() +
    theme(legend.position = "none")

## Calculate lambda per dataset
lambdas = sapply(all_sd_jk, function(x){quantile(x, 0.95, na.rm=T)/median(x, na.rm=T)})

lambdas

mean(lambdas)

median(lambdas)

# Plot distributions of all vectors on the same graph
ggplot(data.frame(lambda=lambdas, dataset=names(lambdas)),
       aes(x = lambdas)) +
    geom_histogram() +
    geom_density() +
    labs(title = "Distribution of lambda accross datasets",
         x = "lambdas",
         y = "Density") +
    theme_minimal() +
    theme(legend.position = "none")

##all_sd_k <- sapply(rds_list_mat, function(k){
##    ## Scale
##    k = log2(k/(1-k))
##
##    get_sd_k <- function(k){
##        ## Calculate the row (=per CpG j) sd
##        sd_j <- rowSds(k, na.rm = T)
##        ## Calculate the median sd in dataset k 
##        return(median(sd_j, na.rm = T))
##    }
##    return(get_sd_k(k))
##})
##
#### Get the top 5%, compare with the median and the mean                                                                                                                                
##mean(all_sd_k)
##
##median(all_sd_k)                                                                                                                                                                        
##top_5_percent <- quantile(all_sd_k, 0.95)
##
#### plot sd_k distribution
##ggplot(data.frame(all_sd_k=all_sd_k), aes(x=all_sd_k)) +
##    geom_density() + 
##    geom_vline(xintercept = top_5_percent, col = "red")+
##    theme_bw()
##
##lambda = top_5_percent/median(all_sd_k)
##
##print(paste0("In Maria's analysis, lambda = ", round(lambda,2)))
```

## Part 4. Maximum likelihood analysis
The equation is:
\deqn{log(P(M_j)) &= \sum_{i=1}^{n} log \biggl(\sum_{Z_j=0,1}\biggl(\sum_{Z_{j,k}=0,1}P(M_{i,j}|Z_{j,k})\times P(Z_{j,k}|Z_j) \biggl)\times P(Z_j)\biggl)}

```{r}
## Maximum likelihood analysis
## log(P(M(j))=sum{i=1:n}(log(sum{Zj=0,1}(sum{Zj,k=0,1}(P(M(i,j)|Z(j,k)) x P(Z(j,k)|Z(j))) x P(Z(j)))))

## Within each dataset k, calculate the median sd of all CpG j

rds_list_mat_preprocessed <- rds_list_mat
rm(rds_list_mat)

## ********** Test: reduce to the first 100 CpGs for all dataset (to rm after test!**********)
## rds_list_mat <- lapply(rds_list_mat, function(m) {
##   if (nrow(m) >= 100) {
##     m[1:100, , drop = FALSE]
##   } else {
##     m
##   }
## })
## 
## saveRDS(rds_list_mat, "/SAN/ghlab/pophistory/Alice/hvCpG_project/data/arrays_human/rds_list_27_TESTtop100.RDS")
## rds_list_mat_preprocessed <- readRDS("/SAN/ghlab/pophistory/Alice/hvCpG_project/data/arrays_human/rds_list_27_TESTtop100.RDS")
## common_cpgs <- unique(unlist(lapply(rds_list_mat_preprocessed, rownames)))
## ********** Test: reduce to the first 100 CpGs for all dataset (to rm after test!**********)
    
scaled_list_mat <- lapply(rds_list_mat_preprocessed, function(k){
    ## Scale
    k = log2(k/(1-k))
})

### mu_jk: named list for each dataset of vectors containing the mean methylation for each CpG j
mu_jk_list <- lapply(scaled_list_mat, function(k){rowMeans(k, na.rm = T)})

### sigma_k: named list for each dataset of elements = the median sd for all CpGs
sigma_k_list <- lapply(scaled_list_mat, function(k){
    ## Calculate the row (=per CpG j) sd
    sd_j <- rowSds(k, na.rm = T)
    ## Calculate the median sd in dataset k 
    return(median(sd_j, na.rm = T))
})

## Get a particular methylation value for dataset k, CpG j and individual i
get_Meth_ijk <- function(k,j,i){
    scaled_list_mat[[k]][rownames(scaled_list_mat[[k]]) %in% j][i]
    ## can be NA if the dataset does not have value for j
    ## or a value for this individual (possibly NA if not sequenced)
}

get_mu_jk <- function(k,j){mu_jk_list[[k]][names(mu_jk_list[[k]]) %in% j]}

get_sd_k <- function(k){sigma_k_list[[k]]}

## Get the different probabilities
Pr_Mij_given_Zjk <- function(i,j,k,Zjk, lambda){
    Mij = get_Meth_ijk(k = k, j = j, i = i)
    mu_jk = get_mu_jk(k = k, j = j)
    sd_k = get_sd_k(k = k)
    if (Zjk==0){
        dnorm(Mij, mean = mu_jk, sd = sd_k)
    } else if (Zjk==1){
        dnorm(Mij, mean = mu_jk, sd = lambda * sd_k)
    }
}

Pr_Zjk_given_Zj <- function(Zj, Zjk, p1, p0){
    if (Zj==0){
        if (Zjk==0){
            p0 # true negative
        } else if (Zjk==1){
            1-p0 # false positive
        }
    } else if (Zj==1){
        if (Zjk==0){
            1-p1 # false negative
        } else if (Zjk==1){ 
            p1 # true positive
        }
    }
}

Pr_Zj <- function(Zj, alpha){
    if (Zj==0){
        1 - alpha
    } else if (Zj==1){
        alpha
    }
}

## alpha is the parameter we want to estimate
getLogLikData <- function(j, lambda = 3, p0 = 0.95, p1 = 0.65, alpha = 0.1){ 
        ## all the is per dataset
        allInds <- function(j){
            sapply(names(scaled_list_mat), function(k) {
                L = length(scaled_list_mat[[k]][rownames(scaled_list_mat[[k]]) %in% j])
                if (L!=0){
                    1:L
                } else {
                    NA
                }})}

        ## names of non empty datasets k for this j
        names_datasets <- names(allInds(j = j)[!is.na(allInds(j = j))])

        log_P_Mj <- sum(sapply(names_datasets, function(k) {
            nind = length(scaled_list_mat[[k]][rownames(scaled_list_mat[[k]]) %in% j])
            sum(sapply(1:nind, function(i){
                log(sum(sapply(c(0,1), function(Zj) {
                    sum(sapply(c(0,1), function(Zjk) {
                        Pr_Mij_given_Zjk(i = i, j = j, k = k, Zjk = Zjk, lambda = lambda) *
                            Pr_Zjk_given_Zj(Zj = Zj, Zjk = Zjk, p1 = p1, p0 = p0)
                    }))  * Pr_Zj(Zj, alpha)
                })))
            }), na.rm=T)
        }))
        return(log_P_Mj)
}

## loglik for one CpG
getLogLikData(common_cpgs[1])

## Next step: optim to find alpha. One or multiple y calculated before. Are CpGs with the top 5% alpha the hvCpGs found by Maria?
```

The analysis is complete.
